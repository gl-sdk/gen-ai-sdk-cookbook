run_id,dataset_name,timestamp,evaluator_name,model_name,generation.score,generation.completeness.score,generation.groundedness.score,generation.redundancy.score,generation.language_consistency.score,generation.refusal_alignment.score,data,metadata
default_simple_qa_data_250605fd,simple_qa_data,2026-01-31T14:25:13.255548,unknown,unknown,1.0,3.0,3.0,1.0,1.0,1.0,"{""question_id"": 1, ""query"": ""What is the capital of France?"", ""generated_response"": ""Paris is the capital of France."", ""expected_response"": ""Paris"", ""retrieved_context"": ""France is a country in Europe. Paris is the capital and largest city of France.""}","{""batch_size"": 10, ""evaluator_parameters"": {""evaluator_0"": {""name"": ""generation"", ""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""run_parallel"": true, ""judge"": null, ""good_thresholds"": {""completeness"": ["">="", 3], ""redundancy"": [""<="", 1], ""groundedness"": ["">="", 3], ""language_consistency"": ["">="", 1], ""refusal_alignment"": ["">="", 1]}, ""bad_thresholds"": {""completeness"": [""<="", 1], ""redundancy"": ["">="", 3], ""groundedness"": [""<="", 1], ""language_consistency"": [""<="", 0], ""refusal_alignment"": [""<="", 0]}, ""metric_0"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""completeness"", ""_evaluation_lock"": null}, ""metric_1"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""groundedness"", ""_evaluation_lock"": null}, ""metric_2"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""redundancy"", ""_evaluation_lock"": null}, ""metric_3"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""language_consistency"", ""_evaluation_lock"": null}, ""metric_4"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""refusal_alignment"", ""_evaluation_lock"": null}}}, ""dataset_name"": ""simple_qa_data""}"
default_simple_qa_data_250605fd,simple_qa_data,2026-01-31T14:25:13.256105,unknown,unknown,1.0,3.0,3.0,1.0,1.0,1.0,"{""question_id"": 2, ""query"": ""What is 2+2?"", ""generated_response"": ""The answer is 4."", ""expected_response"": ""4"", ""retrieved_context"": ""Basic arithmetic: 2+2 equals 4.""}","{""batch_size"": 10, ""evaluator_parameters"": {""evaluator_0"": {""name"": ""generation"", ""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""run_parallel"": true, ""judge"": null, ""good_thresholds"": {""completeness"": ["">="", 3], ""redundancy"": [""<="", 1], ""groundedness"": ["">="", 3], ""language_consistency"": ["">="", 1], ""refusal_alignment"": ["">="", 1]}, ""bad_thresholds"": {""completeness"": [""<="", 1], ""redundancy"": ["">="", 3], ""groundedness"": [""<="", 1], ""language_consistency"": [""<="", 0], ""refusal_alignment"": [""<="", 0]}, ""metric_0"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""completeness"", ""_evaluation_lock"": null}, ""metric_1"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""groundedness"", ""_evaluation_lock"": null}, ""metric_2"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""redundancy"", ""_evaluation_lock"": null}, ""metric_3"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""language_consistency"", ""_evaluation_lock"": null}, ""metric_4"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""refusal_alignment"", ""_evaluation_lock"": null}}}, ""dataset_name"": ""simple_qa_data""}"
default_simple_qa_data_250605fd,simple_qa_data,2026-01-31T14:25:13.256520,unknown,unknown,0.0,1.0,1.0,1.0,1.0,1.0,"{""question_id"": 3, ""query"": ""What is the largest planet in our solar system?"", ""generated_response"": ""Mercury."", ""expected_response"": ""Jupiter"", ""retrieved_context"": ""Jupiter is the fifth planet from the Sun and the largest in the Solar System. ""}","{""batch_size"": 10, ""evaluator_parameters"": {""evaluator_0"": {""name"": ""generation"", ""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""run_parallel"": true, ""judge"": null, ""good_thresholds"": {""completeness"": ["">="", 3], ""redundancy"": [""<="", 1], ""groundedness"": ["">="", 3], ""language_consistency"": ["">="", 1], ""refusal_alignment"": ["">="", 1]}, ""bad_thresholds"": {""completeness"": [""<="", 1], ""redundancy"": ["">="", 3], ""groundedness"": [""<="", 1], ""language_consistency"": [""<="", 0], ""refusal_alignment"": [""<="", 0]}, ""metric_0"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""completeness"", ""_evaluation_lock"": null}, ""metric_1"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""groundedness"", ""_evaluation_lock"": null}, ""metric_2"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""redundancy"", ""_evaluation_lock"": null}, ""metric_3"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""language_consistency"", ""_evaluation_lock"": null}, ""metric_4"": {""batch_status_check_interval"": 30.0, ""batch_max_iterations"": 120, ""name"": ""refusal_alignment"", ""_evaluation_lock"": null}}}, ""dataset_name"": ""simple_qa_data""}"
